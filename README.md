# Neural Network Hyperparameter Implementation and Activation Function Analysis

This project consists of implementing combinations of hyperparameters on a Neural Network to determine what activation
function (Logistic, ReLu, or Tanh) and what hyperparameters with each of those activation functions performs the best.

Below are three Loss/Error/Cost curve graphs for each of the activation functions.

Note: There are more graphs shown in the code file.

# Logistic (Sigmoid) Loss Curves

![My image](https://github.com/Eddie-Carrizales/My-Python-Projects/blob/main/Machine%20Learning%20Projects/Neural%20Network/Activation%20Function%20Graphs/Logistic(Sigmoid)%20Loss%20Curve.png)

# ReLu Loss Curves
![My image](https://github.com/Eddie-Carrizales/My-Python-Projects/blob/main/Machine%20Learning%20Projects/Neural%20Network/Activation%20Function%20Graphs/ReLu%20Loss%20Curve.png)

# Tanh Loss Curves
![My image](https://github.com/Eddie-Carrizales/My-Python-Projects/blob/main/Machine%20Learning%20Projects/Neural%20Network/Activation%20Function%20Graphs/Tanh%20Loss%20Curve.png)
